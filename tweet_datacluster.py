# -*- coding: utf-8 -*-
"""Tweet DataCluster

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15YiAGsvFx3edwiDlm3cisj4IjY5iQIyU
"""

pip install twitter

import tweepy  
import csv
consumer_key = "mdmzMBrRM2nNnTdEoN7jZuxmY"
consumer_secret = "3D6YYKcs6kheu1VjqZzkHOu9Fu5RBXKYOG947t0PGykr39btce"
access_key = "525051045-gOq1C7h2YUbz1gFhRuehci0atHZUPwFC5nwlv0VP"
access_secret = "eXvSLyGa9OrzS6R1EiZ86nxvHGpP0RsxelqyVWkxZqmYV"

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_key, access_secret)
api = tweepy.API(auth)

"""# initialize a list to hold all the tweepy Tweets
alltweets = []

# make initial request for most recent tweets (200 is the maximum allowed count)
new_tweets = api.user_timeline(screen_name=screen_name, count=200)

# save most recent tweets
alltweets.extend(new_tweets)

# save the id of the oldest tweet less one
oldest = alltweets[-1].id - 1

# keep grabbing tweets until there are no tweets left to grab
while len(new_tweets) > 0:
    print
    "getting tweets before %s" % (oldest)

    # all subsiquent requests use the max_id param to prevent duplicates
    new_tweets = api.user_timeline(screen_name=screen_name, count=200, max_id=oldest)

    # save most recent tweets
    alltweets.extend(new_tweets)

    # update the id of the oldest tweet less one
    oldest = alltweets[-1].id - 1

    print
    "...%s tweets downloaded so far" % (len(alltweets))

# transform the tweepy tweets into a 2D array that will populate the csv
outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode("utf-8")] for tweet in alltweets]

a = "mdmzMBrRM2nNnTdEoN7jZuxmY"
b = "3D6YYKcs6kheu1VjqZzkHOu9Fu5RBXKYOG947t0PGykr39btce"
c = "525051045-gOq1C7h2YUbz1gFhRuehci0atHZUPwFC5nwlv0VP"
d = "eXvSLyGa9OrzS6R1EiZ86nxvHGpP0RsxelqyVWkxZqmYV"

#!/usr/bin/python
import tweepy
import csv #Import csv
auth = tweepy.auth.OAuthHandler(a , b)
auth.set_access_token(c , d)

api = tweepy.API(auth)

# Open/create a file to append data to
csvFile = open('result.csv', 'a')

#Use csv writer
csvWriter = csv.writer(csvFile)

for tweet in tweepy.Cursor(api.search,
                           q = "google",
                           since = "2014-02-14",
                           until = "2014-02-15",
                           lang = "en").items():

    # Write a row to the CSV file. I use encode UTF-8
    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])
    print tweet.created_at, tweet.text
csvFile.close()
"""

#Given Direction

import os
import tweepy as tw
import pandas as pd

consumer_key = "mdmzMBrRM2nNnTdEoN7jZuxmY"
consumer_secret = "3D6YYKcs6kheu1VjqZzkHOu9Fu5RBXKYOG947t0PGykr39btce"
access_key = "525051045-gOq1C7h2YUbz1gFhRuehci0atHZUPwFC5nwlv0VP"
access_secret = "eXvSLyGa9OrzS6R1EiZ86nxvHGpP0RsxelqyVWkxZqmYV"

search_words = "#trump"
date_since = "2020-08-15"  #YYYY-MM-DD

tweets = tw.Cursor(api.search,
              q=search_words,
              lang="en",
              since=date_since).items(50)
tweets

for tweet in tweets:
    print(tweet.text)

##############PROGRAM START#################

# Collect tweets
tweets = tw.Cursor(api.search,
                       q=search_words,
                       lang="en",
                       since=date_since).items(50)

# Collect a list of tweets
val = [tweet.text for tweet in tweets]

print(val)

res= val
print(res[0])

#Remove @ and url 
import re
for i in range(len(res)):
  res[i] = re.sub(r'(\s)@\w+', r'\1', res[i])
  res[i] = re.sub(r"http\S+", "", res[i])
  
print(res)



import re
for i in range(len(res)):
  res[i] = re.sub(r'[^\w\s]', '', res[i])

print(res)

tokenised =[]

import nltk
nltk.download('punkt')
for i in range(len(res)):
  word_data = res[i] 
  tokenised.append(nltk.word_tokenize(word_data))

print(tokenised)

print(tokenised[0])

#Remove Stop Words

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stopword = stopwords.words('english')
for i in range(len(res)):
  tokenised[i] = [word for word in tokenised[i] if word not in stopword]

print(tokenised)

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

wordnet_lemmatizer = WordNetLemmatizer()

for i in range(len(res)):
  tokenised[i] = [wordnet_lemmatizer.lemmatize(word) for word in tokenised[i]]

print(tokenised)

#Stemming + Coverting to lowerCase

from nltk.stem import SnowballStemmer
snowball_stemmer = SnowballStemmer('english')

for i in range(len(res)):

  tokenised[i] = [snowball_stemmer.stem(word) for word in tokenised[i]]

print(tokenised)

pip install autocorrect

from autocorrect import spell

for i in range(len(res)):
  tokenised[i] = [spell(w) for w in tokenised[i]]

print(tokenised)

print(len(tokenised))

from nltk.tokenize.treebank import TreebankWordDetokenizer
detokenised=[]
for i in range(len(tokenised)):
  detokenised.append(TreebankWordDetokenizer().detokenize(tokenised[i]))

print(detokenised)
print(len(detokenised))

import string
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer().fit_transform(detokenised)
vectors = vectorizer.toarray()

print(vectors)

csim = cosine_similarity(vectors)
print(csim)

# Import required libraries and modules 
import matplotlib.pyplot as plt 
from sklearn.cluster import Birch 
  

# Creating the BIRCH clustering model 
model = Birch(branching_factor = 50, n_clusters = 3, threshold = 0.5) 

fitted = model.fit(vectors) 
  
# Predict the same data 
pred = model.predict(vectors) 
  
# Creating a scatter plot
print(fitted) 
print(pred)
#plt.show()

print(pred)

print(model)





